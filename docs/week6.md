# 第六周

## 支持向量机

![](https://raw.githubusercontent.com/bighuang624/pic-repo/master/SVM-1.png)

<!--### 优化目标

代价函数：

$$min\_{\theta}C\sum^m\_{i=1}\Big[ y^{(i)}cost\_1(\theta^Tx^{(i)}) + (1-y^{(i)})cost\_0(\theta^Tx^{(i)}) \Big] + \frac{1}{2}\sum^n\_{j=1}\theta^2\_j$$

正则化参数 $C$ 越大，决策边界越会精准分类，因此异常样本造成的影响也会更大。-->

### 直观理解

支持向量机（Support Vector Machine），便是根据训练样本的分布，搜索所有可能的线性分类器中最佳的那个。因为决定直线位置的并非所有的训练数据，而是其中的两个空间**间隔（margin）**最小的两个不同类别的数据点，这种可以用来真正帮助决策最优线性分类模型的数据点叫做**支持向量（support vector）**。

![](https://raw.githubusercontent.com/bighuang624/pic-repo/master/margin-and-support-vector.png)

### 数学原理

在样本空间中，划分超平面可通过以下线性方程来描述：

$$w^Tx+b = 0$$

其中，$w = (w\_1;w\_2;...;w\_d)$ 为**法向量**，决定了超平面的方向；$b$ 为**位移项**，决定了超平面与原点之间的距离。显然，划分超平面 $(w, b)$ 可被法向量 $w$ 和位移 $b$ 确定。

样本空间中任意点 $x$ 到超平面 $(w,b)$ 的距离可写为

$$r=\frac{|w^Tx+b|}{\\|w\\|}$$

假设超平面 $(w,b)$ 能将训练样本正确分类，即对于 $(x\_i, y\_i) \in D$，若 $y\_i = +1$，则有 $w^Tx\_i+b>0$；若 $y\_i = -1$，则有 $w^Tx+b < 0$。令

$$
\begin{equation}
\begin{cases}
w^Tx\_i+b \geq +1, & y\_i=+1 \\\ w^Tx\_i+b \leq -1, & y\_i=-1
\end{cases}
\end{equation}
$$

总存在缩放变换使得上式成立。可以理解为构造了一个安全间距。

显然，**支持向量**使得上式中的等号成立。它们到超平面的距离之和即**间隔**，为

$$\gamma = \frac{2}{\\| w \\|}$$

我们的目标是找到具有最大间隔的划分超平面，即找到满足 $y\_i(w^Tx\_i+b) \geq 1, i = 1, 2, ..., m$ 的参数 $w$ 和 $b$，使得 $\gamma$ 最大，即等价于最小化 $\frac{1}{2}\\| w \\|^2$。

### 核函数

#### 思想

可以用带有**核函数（kernal）**的支持向量机构造复杂的非线性分类器。我们需要定义一个类似于度量相似性的函数，以高斯核函数为例：

$$
\begin{equation}
\begin{split}
f\_1 &= similarity(x, l^{(1)}) \\\ 
&= exp \Big( -\frac{\\| x-l^{(1)} \\|^2}{2\sigma^2} \Big) \\\
&= exp \Big( -\frac{\sum^n\_{j=1}(x\_j - l^{(1)}\_j)^2}{2\sigma^2} \Big)
\end{split}
\end{equation}
$$

其中，$\sigma^2$ 是高斯核函数的参数。当 $x$ 约等于 $l^{(1)}$ 时，$f\_1$ 约等于 1；而当 $x$ 和 $l^{(1)}$ 较远时，$f\_1$ 约等于 0。

通过相似度函数，$l\_1$，$l\_2$，... ，$l\_j$ 每一个标记会定义一个新特征 $f\_1$，$f\_2$，...，$f\_j$。假设有 $\theta\_0 + \theta\_1 f\_1 + \theta\_2 f\_2 + ... + \theta\_j f\_j \geq 0$，我们由此将复杂的非线性决策边界转换为线性。

#### 选取标记点

每个标记点都和每个样本点位置重合，即 $l^{(m)} = x^{(m)}$。则有

$$f\_m^{(i)} = sim(x^{(i)}, l^{(m)})$$

$$f^{(i)} = [f\_0^{(i)}, f\_1^{(i)}, ... ,f\_m^{(i)}]^T$$

$f^{(i)}$ 就是特征向量。

因此，包含核函数的 SVM：

给定 $x$，计算出特征 $f \in \mathbb{R}^{m+1}$，当 $\theta^Tf \geq 0$ 时，预测 $y = 1$。则目标为

$$min\_{\theta}C\sum^m\_{i=1}\Big[ y^{(i)}cost\_1(\theta^Tf^{(i)}) + (1-y^{(i)})cost\_0(\theta^Tf^{(i)}) \Big] + \frac{1}{2}\sum^m\_{j=1}\theta^2\_j$$

一个数学细节是 $\sum\_j \theta^2\_j$ 可以被写为 $\theta^T \theta$（忽略 $\theta\_0$）。而在实现时，通常用 $\theta^T M\theta$，$M$ 是一个依赖于所使用的核函数的矩阵，这是一种略有区别的距离度量方法。这使得 SVM 的实现更有效率。

核函数的思想可以用于例如 Logistic 回归的其他分类器，但是运行十分缓慢，不能使用高级优化技巧。

#### 参数选择

对于参数 $C$，其作用近似于 $\frac{1}{\lambda}$。

* 较大的 $C$：意味着不使用正则化，低偏差高方差
* 较小的 $C$：偏向于欠拟合，高偏差低方差

$\sigma^2$：

* 偏大时：特征 $f\_i$ 更为平滑，高偏差低方差
* 偏小时：特征 $f\_i$ 更不平滑，低偏差高方差

### 使用 SVM

使用 SVM 时，需要确定参数 $C$ 和核函数。

可以不使用核函数，也称使用线性核函数，则 $\theta^Tx \geq 0$。当特征数目较多而样本数较少时，这样可以避免去在一个非常高维的空间中拟合过于复杂的非线性函数，减缓过拟合。

当特征数目较少而训练样本较多时，想用核函数拟合相当复杂的非线性决策边界，高斯核函数是个不错的选择。如果决定使用高斯核函数，还需要选择 $\sigma^2$。

如果有大小很不一样的特征变量，在使用高斯核函数前，要注意将特征变量大小按比例归一化。

最常用的两个核函数是线性核函数和高斯核函数。不是所有提出来的相似性函数都是有效的，需要满足默塞尔定理（Mercer's Theorem），使得用于求解 $\theta$ 的数值优化技巧能够使用。

其他可以使用的核函数有：

- 多项式函数（Polynomial kernel）：$k(x, l) = (X^Tl + m)^n$。通常效果较差，常用于 $x$ 和 $l$ 都是严格的非负数。
- 字符串核函数（String kernel）：用于输入数据是字符串时。
- 卡方核函数（chi-square kernel）
- 直方相交核函数（histogram kernel）

如何选择使用 Logistic 回归还是 SVM？

- 当特征数量远大于训练样本数，一般使用 Logistic 回归，或使用线性核函数的 SVM。
- 当特征数量较少，训练样本数中等时，一般使用带高斯核函数的 SVM。
- 当特征数量较少，训练样本数非常多（一般大于十万），一般先增加更多特征变量，然后使用 Logistic 回归或者使用线性核函数的 SVM。

而设计较好的神经网络可能能获得较好的效果，但训练速度较慢。并且 SVM 的优化是一种凸优化问题，不会有困扰神经网络的局部最优问题。

## 无监督学习

在监督学习中，我们有一系列标签，然后用假设函数去拟合它。作为对比，在无监督学习中，数据不带有标签。我们通过这些数据找到一些隐含在数据中的结构。聚类算法是一种典型的无监督学习算法。

### K-Means 算法

K-means 是一种经典的聚类算法，其需要的输入包括聚类数目 $K$ 和训练集 $\big\\{x^{(1)}, x^{(2)}, ..., x^{(m)}\big\\}$。

步骤：

1. 随机初始化 $K$ 个聚类中心 $\mu\_1, \mu\_2, ..., \mu\_K \in \mathbb{R}^{n}$；
2. 将训练集中的每个数据点分配给离它最近的聚类中心，最终形成 $K$ 个聚类；
3. 对于每个聚类，将聚类中所有数据点的平均点作为新的聚类中心；
4. 重复第 2 步和第 3 步直到收敛。

可能在过程中出现没有任何数据点的聚类中心，这种情况下一般直接移除，或者如果确实需要 $K$ 个聚类，则可以重新随机初始化一个聚类中心。

#### 优化目标

规定下列符号：

- $c^{(i)}$：$x^{(i)}$ 被分配给的聚类的序号；
- $\mu\_k$：第 $k$ 个聚类中心；
- $\mu\_{c^{(i)}}$：$x^{(i)}$ 被分配给的聚类的中心。

则有代价函数作为优化目标：

$$J(c^{(1)}, ... , c^{(m)}, \mu\_1, ... , \mu\_K) = \frac{1}{m}\sum^m\_{i=1} \\| x^{(i)} - \mu\_{c^{(i)}} \\|^2$$ 

因此，K-means 算法就是要找到能使 $J(c^{(1)}, ... , c^{(m)}, \mu\_1, ... , \mu\_K)$ 最小的参数 $c^{(i)}$ 和 $\mu\_K$。

#### 随机初始化

一个更好的策略是不再从空间中随机选择点，而是从训练数据中随机选择以初始化聚类中心。

K-means 算法最终可能因为选取了不同的初始化点而收敛得到不同的结果。因此，可能落到局部最优。我们可以尝试多次随机初始化，并选择能够使代价函数最小的聚类结果。

#### 选择聚类数量

通过改变 $K$ 值来得到一张代价函数下降的曲线图，并选择曲线的拐点的横坐标作为合适的 $K$ 值被称为“肘部法则（Elbow method）”，但不是所有时候得到的曲线都有明显的拐点。

<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
   tex2jax: {inlineMath: [ ['$', '$'] ],
         displayMath: [ ['$$', '$$']]}
 });
</script>

<script src="https://cdn.bootcss.com/mathjax/2.7.4/latest.js?config=default"></script>